{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2b8e484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "08f19271",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/bkohler/Downloads/rubric-2.json\", \"r\") as file:\n",
    "    rubric = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b9abe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_requirements(task, prefix=''):\n",
    "    requirements = []\n",
    "\n",
    "    def recurse(t, path):\n",
    "        req = t.get(\"requirements\", \"\")\n",
    "        if req:\n",
    "            requirements.append(f\"{path}: {req}\")\n",
    "        for i, sub in enumerate(t.get(\"sub_tasks\", []), start=1):\n",
    "            recurse(sub, f\"{path}.{i}\" if path else str(i))\n",
    "\n",
    "    recurse(task, prefix)\n",
    "    return requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "26ea20d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_list = flatten_requirements(rubric, prefix='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "293f7d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "': The paper \"Bridging Data Gaps in Diffusion Models with Adversarial Noise-Based Transfer Learning\" has been reproduced. 1: Algorithm 1 for training DPMs with Adversarial Noise-based Transfer has been implemented. 1.1: Code has been written to train a binary classifier to predict whether an input $x_t$ originates from the source domain or target domain. 1.2: A binary classifier has been trained to predict whether an input $x_t$ originates from the source domain or target domain. 1.3: The adaptor module from Noguchi & Harada, 2019 has been implemented. 1.4: Code has been written to select training samples from the target dataset, a time-step selected randomly, and standard Gaussian noise for each sample. 1.5: Training samples are drawn from the target dataset, each paired with a randomly selected timestep and standard Gaussian noise. 1.6: Code has been written to compute the adaptive inner maximum as defined in Equation 7. 1.7: The adaptive inner maximum has been computed as defined in Equation 7. 1.8: Code has been written to compute the similarity guided loss as defined in Equation 5. 1.9: The similarity guided loss has been computed as defined in Equation 5. 1.10: Code has been written to update the adaptor module parameters via gradient descent with similarity-guided loss and adversarial noise as the objective. The objective function is defined in Equation 8. 1.11: The adaptor module parameters are updated via gradient descent with similarity-guided loss and adversarial noise as the objective. The objective function is defined in Equation 8. 2: The experimental setup has been implemented, as described in Section 5. 2.1: The experimental setup to evaluate the DDPM and LDM models has been replicated. 2.1.1: Code has been written to fine-tune a pre-trained LDM in the shift module of the U-net. The pre-trained DPM and autoencoders in LDM are kept fixed. 2.1.2: A pre-trained LDM has been fine-tuned in the shift module of the U-net. The pre-trained DPM and autoencoders in LDM are kept fixed. 2.1.3: Code has been written to initialize the parameters of the adapter layer to zero. 2.1.4: The parameters of the adapter layer are initialized to zero. 2.1.5: Code has been written to set the hyper-parameter gamma for similarity-guided training to 5. 2.1.6: The hyper-parameter gamma for similarity-guided training is set to 5. 2.1.7: Code has been written to fine-tune a pre-trained model on ImageNet with a binary classifier head on 10 target domain images. 2.1.8: A pre-trained model on ImageNet is fine-tuned with a binary classifier head on 10 target domain images. 2.1.9: Code has been written to set the hyperparameters $J$ and $\\\\omega$ to 10 and 0.02, respectively, for adversarial noise selection. 2.1.10: The hyperparameters $J$ and $\\\\omega$ are set to 10 and 0.02, respectively, for adversarial noise selection. 2.1.11: The learning rate is set to 0.00005 for DDPM and 0.00001 for LDM. Both models are trained for 300 iterations and a batch size of 40. 2.1.12: The learning rate is set to 0.00005 for DDPM and 0.00001 for LDM. Both models are trained for 300 iterations and a batch size of 40. 2.2: The evaluation metrics have been implemented. 2.2.1: The Intra-LPIPS metric has been implemented. 2.2.1.1: Code has been written to generate 1,000 images from the models. 2.2.1.2: 1,000 images have been generated from the models. 2.2.1.3: Code has been written to assign each image to the training sample with the smallest LPIPS distance. 2.2.1.4: Each image has been assigned to the training sample with the smallest LPIPS distance. 2.2.1.5: The average pair-wise distance within each cluster has computed. The average score from each cluster has been calculated as Intra-LPIPS metric. 2.2.1.6: The average pair-wise distance within each cluster has computed. The average score from each cluster has been calculated as Intra-LPIPS metric. 2.2.2: The FID metric has been implemented. 2.3: The target datasets have been selected for their corresponding source datasets. 2.3.1: Code has been written to load Babies, Sunglasses, Raphael Peale, Sketches, and face paintings from the dataset FFHQ. 2.3.2: Babies, Sunglasses, Raphael Peale, Sketches, and face paintings have been loaded from the dataset FFHQ. 2.3.3: Haunted Houses and Landscape drawings have been loaded from the LSUN dataset. 2.3.4: Haunted Houses and Landscape drawings have been loaded from the LSUN dataset. 2.4: The baseline models used in Table 1 have been implemented by adapting pre-existing implementations from the StyleGAN2 codebase. 2.4.1: TGAN has been implemented by adapting the StyleGAN2 codebase. 2.4.2: TGAN+ADA has been implemented by adapting the StyleGAN2 codebase. 2.4.3: EWC has been implemented by adapting the StyleGAN2 codebase. 2.4.4: CDC has been implemented by adapting the StyleGAN2 codebase. 2.4.5: DCL has been implemented by adapting the StyleGAN2 codebase. 2.4.6: DDPM-PA has been implemented by adapting the StyleGAN2 codebase. 3: Figure 2 has been replicated. 3.1: Code has been written to train a diffusion model to generate data from a 2-dimensional Gaussian distribution with mean [1, 1] and unit variance. 3.2: A diffusion model has been trained to generate data from a 2-dimensional Gaussian distribution with mean [1, 1] and unit variance. 3.3: Code has been written to transfer the trained model to generate samples from a 2-dimensional Gaussian distribution with a mean of [-1, -1] and unit variance using three methods: DDPM, DDPM-ANT w/o AN, and DDPM-ANT. 3.4: The trained model has been transferred to generate samples from a 2-dimensional Gaussian distribution with a mean of [-1, -1] and unit variance using three methods: DDPM, DDPM-ANT w/o AN, and DDPM-ANT. 3.5: Figure 2a has been replicated. 3.5.1: Code has been written to calculate the gradient direction used as reference using 10,000 samples. 3.5.2: The gradient direction used as reference has been calculated using 10,000 samples. 3.5.3: Code has been written to calculate the gradient of the output layer during the first iteration with 10-shot samples for DDPM. 3.5.4: The gradient of the output layer during the first iteration has been calculated with 10-shot samples for DDPM. 3.5.5: Code has been written to calculate the gradient of the output layer during the first iteration with 10-shot samples for DDPM fine-tuned using similarity-guided training only (DDPM-ANT w/o AN). 3.5.6: The gradient of the output layer during the first iteration has been calculated with 10-shot samples for DDPM fine-tuned using similarity-guided training only (DDPM-ANT w/o AN). 3.5.7: Code has been written to calculate the gradient of the output layer during the first iteration with 10-shot samples for DDPM fine-tuned using similarity-guided training and adversarial noise selection (DDPM-ANT as defined by Equation 8). 3.5.8: The gradient of the output layer during the first iteration has been calculated with 10-shot samples for DDPM fine-tuned using similarity-guided training and adversarial noise selection (DDPM-ANT as defined by Equation 8). 3.5.9: The gradients computed for DDPM show the largest angular deviation from the reference gradient. Using DDPM without AN shows a decrease in the angular difference, and the closest angular difference between the direction of the gradient and the reference was achieved by the proposed method DDPM-ANT. 3.6: Figure 2b and Figure 2c have been replicated. 3.6.1: Code has been written to generate 20,000 samples using the DDPM model. 3.6.2: The DDPM model has been used to generate 20,000 samples. 3.6.3: Code has been written to generate 20,000 samples using the DDPM-ANT model. 3.6.4: The DDPM-ANT model has been used to generate 20,000 samples. 3.6.5: The samples generated by DDPM-ANT has a larger concentration of samples around -1,  which is the mean of the target distribution, than DDPM. 4: Figure 3 has been replicated. 4.1: The 10-shot image generation for adapting LSUN Church to Landscape drawings have been replicated. 4.1.1: Code has been written to perform 10-shot image generation, adapting the LSUN Church dataset to Landscape drawings using the CDC model. 4.1.2: The CDC model has been used to perform 10-shot image generation, adapting the LSUN Church dataset to Landscape drawings. 4.1.3: Code has been written to perform 10-shot image generation, adapting the LSUN Church dataset to Landscape drawings using the DCL model. 4.1.4: The DCL model has been used to perform 10-shot image generation, adapting the LSUN Church dataset to Landscape drawings. 4.1.5: Code has been written to perform 10-shot image generation, adapting the LSUN Church dataset to Landscape drawings using the DDPM-PA model. 4.1.6: The DDPM-PA model has been used to perform 10-shot image generation, adapting the LSUN Church dataset to Landscape drawings. 4.1.7: Code has been written to perform 10-shot image generation, adapting the LSUN Church dataset to Landscape drawings using the DDPM-ANT model. 4.1.8: The DDPM-ANT model has been used to perform 10-shot image generation, adapting the LSUN Church dataset to Landscape drawings. 4.1.9: Code has been written to perform 10-shot image generation, adapting the LSUN Church dataset to Landscape drawings using the LDM-ANT model. 4.1.10: The LDM-ANT model has been used to perform 10-shot image generation, adapting the LSUN Church dataset to Landscape drawings. 4.1.11: The images generated by DDPM-ANT and LDM-ANT show better results at capturing the style of landscapes and representing buildings from the source domain. 4.1.12: The images generated by CDC and DCL capture the color scheme of the target domain, but fail to capture the structure of the source domain. 4.1.13: The images generated by DDPM-PA capture the structure of the source domain, but fail to capture the color scheme of the target domain. 4.2: The 10-shot image generation for adapting FFHQ to Raphael\\'s paintings has been replicated. 4.2.1: Code has been written to perform 10-shot image generation, adapting the FFHQ dataset to Raphael\\'s paintings using the CDC model. 4.2.2: The CDC model has been used to perform 10-shot image generation, adapting the FFHQ dataset to Raphael\\'s paintings. 4.2.3: Code has been written to perform 10-shot image generation, adapting the FFHQ dataset to Raphael\\'s paintings using the DCL model. 4.2.4: The DCL model has been used to perform 10-shot image generation, adapting the FFHQ dataset to Raphael\\'s paintings. 4.2.5: Code has been written to perform 10-shot image generation, adapting the FFHQ dataset to Raphael\\'s paintings using the DDPM-PA model. 4.2.6: The DDPM-PA model has been used to perform 10-shot image generation, adapting the FFHQ dataset to Raphael\\'s paintings. 4.2.7: Code has been written to perform 10-shot image generation, adapting the FFHQ dataset to Raphael\\'s paintings using the DDPM-ANT model. 4.2.8: The DDPM-ANT model has been used to perform 10-shot image generation, adapting the FFHQ dataset to Raphael\\'s paintings. 4.2.9: Code has been written to perform 10-shot image generation, adapting the FFHQ dataset to Raphael\\'s paintings using the LDM-ANT model. 4.2.10: The LDM-ANT model has been used to perform 10-shot image generation, adapting the FFHQ dataset to Raphael\\'s paintings. 4.2.11: The images generated by CDC and DCL capture the style of the target domain, but the generated images are blurry or distorted. 4.2.12: The images generated by DDPM-PA are clear, but fail to capture the style of the target domain. 4.2.13: The images generated by DDPM-ANT and LDM-ANT results at better capture the style of the target domain, while being less blurry or distorted than those generated by CDC and DCL. 5: Table 1 has been replicated. 5.1: The results for TGAN have been replicated. 5.1.1: Code has been written to update all parameters of TGAN during fine-tuning. 5.1.2: All parameters of TGAN were updated during fine-tuning. 5.1.3: The Intra-LPIPS score for the 10-shot image generation adapting FFHQ to Babies using TGAN was approximately 0.510. 5.1.4: The Intra-LPIPS score for the 10-shot image generation adapting FFHQ to Sunglasses using TGAN was approximately 0.550. 5.1.5: The Intra-LPIPS score for the 10-shot image generation adapting FFHQ to Raphael\\'s painting using TGAN was approximately 0.533. 5.1.6: The Intra-LPIPS score for the 10-shot image generation adapting LSUN Church to Haunted houses using TGAN was approximately 0.585. 5.1.7: The Intra-LPIPS score for the 10-shot image generation adapting LSUN Church to Landscape drawings using TGAN was approximately 0.601. 5.2: The results for TGAN+ADA have been replicated. 5.2.1: Code has been written to update all parameters of TGAN+ADA during fine-tuning. 5.2.2: All parameters of the model were updated during fine-tuning of TGAN+ADA. 5.2.3: The Intra-LPIPS score for the 10-shot image generation adapting FFHQ to Babies using TGAN+ADA was approximately 0.546.  5.2.4: The Intra-LPIPS score for the 10-shot image generation adapting FFHQ to Sunglasses using TGAN+ADA was approximately 0.571. 5.2.5: The Intra-LPIPS score for the 10-shot image generation adapting FFHQ to Raphael\\'s painting using TGAN+ADA was approximately 0.546.  5.2.6: The Intra-LPIPS score for the 10-shot image generation adapting LSUN Church to Haunted houses using TGAN+ADA was approximately 0.615. 5.2.7: The Intra-LPIPS score for the 10-shot image generation adapting LSUN Church to Landscape drawings using TGAN+ADA was approximately 0.643. 5.3: The results for EWC have been replicated. 5.3.1: Code has been written to update all parameters of EWC during fine-tuning. 5.3.2: All parameters of the model were updated during fine-tuning of EWC. 5.3.3: The Intra-LPIPS score for the 10-shot image generation adapting FFHQ to Babies using EWC was approximately 0.560.  5.3.4: The Intra-LPIPS score for the 10-shot image generation adapting FFHQ to Sunglasses using EWC was approximately 0.550.  5.3.5: The Intra-LPIPS score for the 10-shot image generation adapting FFHQ to Raphael\\'s painting using EWC was approximately 0.541.  5.3.6: The Intra-LPIPS score for the 10-shot image generation adapting LSUN Church to Haunted houses using EWC was approximately 0.579. 5.3.7: The Intra-LPIPS score for the 10-shot image generation adapting LSUN Church to Landscape drawings using EWC was approximately 0.596. 5.4: The results for CDC have been replicated. 5.4.1: Code has been written to update all parameters of CDC during fine-tuning. 5.4.2: All parameters of the model were updated during fine-tuning of CDC. 5.4.3: The Intra-LPIPS score for the 10-shot image generation adapting FFHQ to Babies using CDC was approximately 0.583.  5.4.4: The Intra-LPIPS score for the 10-shot image generation adapting FFHQ to Sunglasses using CDC was approximately 0.581.  5.4.5: The Intra-LPIPS score for the 10-shot image generation adapting FFHQ to Raphael\\'s painting using CDC was approximately 0.564.  5.4.6: The Intra-LPIPS score for the 10-shot image generation adapting LSUN Church to Haunted houses using CDC was approximately 0.620. 5.4.7: The Intra-LPIPS score for the 10-shot image generation adapting LSUN Church to Landscape drawings using CDC was approximately 0.674. 5.5: The results for DCL have been replicated. 5.5.1: Code has been written to update all parameters of DCL during fine-tuning. 5.5.2: All parameters of the model were updated during fine-tuning of DCL. 5.5.3: The Intra-LPIPS score for the 10-shot image generation adapting FFHQ to Babies using DCL was approximately 0.579.  5.5.4: The Intra-LPIPS score for the 10-shot image generation adapting FFHQ to Sunglasses using DCL was approximately 0.574.  5.5.5: The Intra-LPIPS score for the 10-shot image generation adapting FFHQ to Raphael\\'s painting using DCL was approximately 0.558.  5.5.6: The Intra-LPIPS score for the 10-shot image generation adapting LSUN Church to Haunted houses using DCL was approximately 0.616. 5.5.7: The Intra-LPIPS score for the 10-shot image generation adapting LSUN Church to Landscape drawings using DCL was approximately 0.626. 5.6: The results for DDPM-PA have been replicated. 5.6.1: Code has been written to update all parameters of DDPM-PA during fine-tuning. 5.6.2: All parameters of the models were updated during fine-tuning of DDPM-PA. 5.6.3: The Intra-LPIPS score for the 10-shot image generation adapting FFHQ to Babies using DDPM-PA was approximately 0.599.  5.6.4: The Intra-LPIPS score for the 10-shot image generation adapting FFHQ to Sunglasses using DDPM-PA was approximately 0.604.  5.6.5: The Intra-LPIPS score for the 10-shot image generation adapting FFHQ to Raphael\\'s painting using DDPM-PA was approximately 0.581.  5.6.6: The Intra-LPIPS score for the 10-shot image generation adapting LSUN Church to Haunted houses using DDPM-PA was approximately 0.628. 5.6.7: The Intra-LPIPS score for the 10-shot image generation adapting LSUN Church to Landscape drawings using DDPM-PA was approximately 0.706. 5.7: The results for DDPM-ANT have been replicated. 5.7.1: Only 1.3% of the total number of parameters of the model were updated during fine-tuning of DDPM-ANT. 5.7.2: The Intra-LPIPS score for the 10-shot image generation adapting FFHQ to Babies using DDPM-ANT was approximately 0.592.  5.7.3: The Intra-LPIPS score for the 10-shot image generation adapting FFHQ to Sunglasses using DDPM-ANT was approximately 0.613.  5.7.4: The Intra-LPIPS score for the 10-shot image generation adapting FFHQ to Raphael\\'s painting using DDPM-ANT was approximately 0.621.  5.7.5: The Intra-LPIPS score for the 10-shot image generation adapting LSUN Church to Haunted houses using DDPM-ANT was approximately 0.648. 5.7.6: The Intra-LPIPS score for the 10-shot image generation adapting LSUN Church to Landscape drawings using DDPM-ANT was approximately 0.723. 5.8: The results for LDM-ANT have been replicated. 5.8.1: Only 1.6% of the total number of parameters of the model were updated during fine-tuning of LDM-ANT.  5.8.2: The Intra-LPIPS score for the 10-shot image generation adapting FFHQ to Babies using LDM-ANT was approximately 0.601.  5.8.3: The Intra-LPIPS score for the 10-shot image generation adapting FFHQ to Sunglasses using LDM-ANT was approximately 0.613.  5.8.4: The Intra-LPIPS score for the 10-shot image generation adapting FFHQ to Raphael\\'s painting using LDM-ANT was approximately 0.592.  5.8.5: The Intra-LPIPS score for the 10-shot image generation adapting LSUN Church to Haunted houses using LDM-ANT was approximately 0.653. 5.8.6: The Intra-LPIPS score for the 10-shot image generation adapting LSUN Church to Landscape drawings using LDM-ANT was approximately 0.738. 6: Table 2 has been replicated. 6.1: The results for TGAN have been replicated. 6.1.1: The FID score using TGAN for 10-shot transfer from FFHQ to Babies is approximately 104. 6.1.2: The FID score using TGAN for 10-shot transfer from FFHQ to Sunglasses is approximately 55. 6.2: The results for ADA have been replicated.  6.2.1: The FID score using ADA for 10-shot transfer from FFHQ to Babies is approximately 102. 6.2.2: The FID score using ADA for 10-shot transfer from FFHQ to Sunglasses is approximately 53. 6.3: The results for EWC have been replicated.  6.3.1: The FID score using EWC for 10-shot transfer from FFHQ to Babies is approximately 87. 6.3.2: The FID score using EWC for 10-shot transfer from FFHQ to Sunglasses is approximately 59. 6.4: The results for CDC have been replicated.  6.4.1: The FID score using CDC for 10-shot transfer from FFHQ to Babies is approximately 74. 6.4.2: The FID score using CDC for 10-shot transfer from FFHQ to Sunglasses is approximately 42. 6.5: The results for DCL have been replicated.  6.5.1: The FID score using DCL for 10-shot transfer from FFHQ to Babies is approximately 52. 6.5.2: The FID score using DCL for 10-shot transfer from FFHQ to Sunglasses is approximately 38. 6.6: The results for DDPM-PA have been replicated.  6.6.1: The FID score using DDPM-PA for 10-shot transfer from FFHQ to Babies is approximately 48. 6.6.2: The FID score using DDPM-PA for 10-shot transfer from FFHQ to Sunglasses is approximately 34. 6.7: The results for ANT have been replicated.  6.7.1: The FID score using ANT for 10-shot transfer from FFHQ to Babies is approximately 46. 6.7.2: The FID score using ANT for 10-shot transfer from FFHQ to Sunglasses is approximately 20. 7: Figure 4 has been replicated. 7.1: Code has been written to fine-tune the DPM model on a 10-shot sunglasses dataset for 300 iterations. 7.2: The DPM model was fine-tuned on a 10-shot sunglasses dataset for 300 iterations. 7.3: Code has been written to fine-tune the DPM model using an adaptor layer on a 10-shot sunglasses dataset for 300 iterations, updating only the adaptor layer. 7.4: The DPM model was fine-tuned using an adaptor layer on a 10-shot sunglasses dataset for 300 iterations, updating only the adaptor layer. 7.5: Code has been written to fine-tune the DPM model using only similarity guided training on a 10-shot sunglasses dataset for 300 iterations. 7.6: The DPM model was fine-tuned using only similarity guided training on a 10-shot sunglasses dataset for 300 iterations. 7.7: Code has been written to fine-tune the DPM model using the proposed DPM-ANT strategy on a 10-shot sunglasses dataset for 300 iterations. 7.8: The DPM model was fine-tuned using the proposed DPM-ANT strategy on a 10-shot sunglasses dataset for 300 iterations. 7.9: Code has been written to fine-tune the DPM model using the proposed DPM-ANT strategy on a 10-shot sunglasses dataset for 300 iterations. 7.10: DPM-ANT generated images show better quality and detail than the other ones. 7.11: The adaptor results have the highest FID score, followed by the baseline results. DPM-ANT w/o AN achieve a lower FID score while the proposed DPM-ANT has the smallest FID score. 7.12: Both DPM-ANT w/o AN and the proposed DPM ANT successfully transfer sunglasses to all images. The baseline and adaptor methods both fail to transfer sunglasses to some of the images generated. 8: Table 3 has been replicated. 8.1: The results for the 10-shot classifier have been replicated. 8.1.1: Code has been written to use the DPM-ANT model to transfer FFHQ to Sunglasses, and train a classifier on 10 adapted images. 8.1.2: The DPM-ANT model was used to transfer FFHQ to Sunglasses, and a classifier was trained on 10 adapted images. 8.1.3: The intra-LPIPS score of the 10-shot classifier is approximately 0.613. 8.1.4: The FID score of the 10-shot classifier is approximately 20. 8.2: The results for the 100-shot classifier have been replicated. 8.2.1: Code has been written to use the DPM-ANT model to transfer FFHQ to Sunglasses, and train a classifier on 100 adapted images. 8.2.2: The DPM-ANT model was used to transfer FFHQ to Sunglasses, and a classifier was trained on 100 adapted images. 8.2.3: The intra-LPIPS score of the 100-shot classifier is approximately 0.637. 8.2.4: The FID score of the 100-shot classifier is approximately 22.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(req_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6064d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/bkohler/Downloads/rubric.json\", \"r\") as file:\n",
    "    rubric_2 = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7cdd9de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[': The Simformer model, training process and inference process has been implemented',\n",
       " '1: The Simformer architecture and tokenizer has been implemented',\n",
       " '1.1: The tokenizer for the Simformer has been implemented',\n",
       " '1.1.1: The tokenizer correctly tokenizes and embeds the identifiers',\n",
       " '1.1.1.1: Each unique variable has an integer identifier. Multiple observations of the same variable share the same identifier',\n",
       " '1.1.1.2: The tokenizer uses learnable vector embeddings to embed each identifier',\n",
       " '1.1.2: The embedding for a single value is created by repeating the scalar value to match the desired dimensionality. For example, to embed the value 1 to desired dimensionality N, we would have a vector [1, 1, ...., 1] of length N',\n",
       " '1.1.3: The tokenizer correctly tokenizes and embeds the metadata (if required); the tokenizer applies a learnable linear mapping of a random Fourier embedding of the elements in the index set to the desired dimensionality.',\n",
       " '1.1.4: The tokenizer uses learnable vector embeddings to embed each value in a condition state - \"True\" values are mapped to a shared learnable vector embedding, whereas \"False\" values are mapped to zeros (of the desired dimensionality)',\n",
       " '1.1.5: For each input, the tokenizer concatenates the embeddings of the identifier, value, metadata (if used), and condition state in that order',\n",
       " '1.1.6: The tokenizer takes inputs: a sequence of scalar values, a sequence of integer variable identifiers, a condition mask $M_C$, and optional metadata if required. It embeds these inputs into a sequence of tokens, each of equal dimensionality',\n",
       " '1.2: The Simformer architecture has been implemented',\n",
       " '1.2.1: The Simformer model is a slightly modified vanilla encoder-only transformer following the implementation proposed by (Vaswani et al., 2017). The only modification is that the decoder is a single linear layer that produces a single scalar score for each variable token in the input sequence. The Simformer model is described in Section 3, Figure 2, and Appendix A.1',\n",
       " '1.2.2: Diffusion time is embedded as a random Gaussian Fourier embedding, and a linear projection of diffusion time is added to the output of each feed-forward block in the transformer',\n",
       " '1.2.3: The Simformer takes inputs: the diffusion time $t$, a sequence of tokens from the tokenizer, and an attention mask. These are projected to a sequence of scalar outputs, representing the marginal scores of the diffusion process at time $t$.',\n",
       " '2: The condition and attention masks are correctly computed for each sample passed to the Simformer',\n",
       " '2.1: During training, for each training sample, the condition mask $M_C$ is randomly sampled as either 1) the joint distribution, where $M_C=[0, 0, ..., 0]$, 2) the posterior distribution where data variables are observed and parameters are unobserved, 3) the likelihood function where data variables are unobserved and parameter variables are observed, 4) a Bernoulli distribution with p=0.3 (resampled for each element), 5) a Bernoulli distribution with p=0.7 (resampled for each element)',\n",
       " '2.2: The attention mask $M_E$ is correctly computed for each sample passed to the Simformer',\n",
       " '2.2.1: $M_E$ is selected to be undirected, directed, or fully dense',\n",
       " '2.2.2: If $M_E$ is selected to be fully dense, every token is allowed to attend to every other token',\n",
       " '2.2.3: The attention mask $M_E$ is correctly computed for the undirected and directed cases',\n",
       " '2.2.3.1: For both undirected and directed cases, the attention mask $M_E$ is computed to capture the known dependencies of the current task. Specifically, each task provides $M_E$ as given by the adjacency matrix of a directed/undirected graphical model with the diagonal set to True.',\n",
       " '2.2.3.2: If $M_E$ is directed it must be updated for a given $M_C$. The algorithm proposed by Webb at al. (2018) is used to add the minimal number of edges required to represent additional dependencies from conditioning as specified in $M_C$',\n",
       " '3: The code for training the Simformer model has been implemented',\n",
       " '3.1: When training the Simformer, for each training sample $\\\\hat{x}_0$, the noise level $t$ is sampled in the range [1e-5, 1] to generate a (partially) noisy sample $\\\\hat{\\\\mathbf{x}}_t^{M_C} = (1 - M_C) \\\\cdot \\\\hat{\\\\mathbf{x}}_t + M_C \\\\cdot \\\\hat{\\\\mathbf{x}}_0$ i.e. variables that we want to condition on remain clean.',\n",
       " '3.2: The loss function for the Simformer is correctly defined',\n",
       " '3.2.1: A diffusion model loss is used that targets (un)conditional marginal score $\\\\nabla_{\\\\mathbf{x}_t^{\\\\text{unobserved}}}\\\\,\\\\log p_t(\\\\mathbf{x}_t^{\\\\text{unobserved}} \\\\mid \\\\mathbf{x}^{\\\\text{observed}})$ as defined by the condition mask $M_C$ and p(x).',\n",
       " '3.2.2: As defined in Section 3.3, for each (partially) noisy training sample $\\\\hat{x}_t^{M_c}$, the Simformer loss is defined as: $\\\\ell(\\\\phi, M_C, t, \\\\hat{\\\\mathbf{x}}_0, \\\\hat{\\\\mathbf{x}}_t) = (1-M_C)\\\\cdot \\\\left(s_\\\\phi^{M_E}(\\\\hat{\\\\mathbf{x}}_t^{M_C}, t) - \\\\nabla_{\\\\hat{\\\\mathbf{x}}_t} \\\\log p_t(\\\\hat{\\\\mathbf{x}}_t|\\\\hat{\\\\mathbf{x}}_0)\\\\right)$, where $s_\\\\phi^{M_E}$ denotes the score model equipped with a specific attention mask $M_E$',\n",
       " '3.2.3: The Simformer loss is only computed over samples that are unobserved, i.e., they have a value of 0 in $M_C$',\n",
       " '3.2.4: The total Simformer loss per mini-match is given by a weighted sum, as given by a positive weighting function $\\\\lambda(t)$ e.g. $\\\\lambda(t)=g(t)^2$ where $g(t)$ is the diffusion coefficient of the VESDE.',\n",
       " '4: Code for sampling arbitrary conditionals from a trained Simformer model has been implemented',\n",
       " '4.1: When performing inference using Simformer, the reverse diffusion process is run on all unobserved variables. Observed variables are kept constant at their initial values',\n",
       " '4.2: Euler-Maruyama discretization has been implemented to run the diffusion process backward',\n",
       " '4.2.1: Euler-Maruyama discretization has been implemented to run the diffusion process backward for Simformer',\n",
       " '4.2.2: 500 steps are used in the Euler-Maruyama discretization process for Simformer',\n",
       " '5: The Simformer supports conditioning on intervals.',\n",
       " '5.1: Algorithm 1 has been implemented such that conditioning on intervals can optionally be selected',\n",
       " '5.1.1: For conditioning on intervals, the self-recurrence is implemented such that a mutable hyperparameter $r$ specifies how many times future points undergo a forward diffusion step followed by a reverse step, incrementally refining each sample.',\n",
       " '5.1.2: For conditioning on intervals, the constraint function is defined as $c(\\\\hat{x}) = \\\\hat{x} - u$, where $u$ is a mutable hyperparameter',\n",
       " '5.1.3: For conditioning on intervals, the scaling function $s(t)$ is defined as $s(t)=\\\\frac{1}{\\\\sigma(t)^2)|$, i.e., inversely proportional to the variance',\n",
       " '5.1.4: Algorithm 1 has been fully implemented to sample from conditions specified by the constraint function.',\n",
       " '5.2: Conditioning on intervals can optionally be used when sampling']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_requirements(rubric_2[\"sub_tasks\"][1], prefix='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a42f54ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Variance Exploding SDE (VESDE) is implemented, as described in A2.1',\n",
       " 'The Simformer model, training process and inference process has been implemented',\n",
       " 'Baseline methods Neural Posterior Estimation (NPE), Neural Ratio Estimation (NRE), and Neural Likelihood Estimation (NLE) are implemented',\n",
       " 'The benchmark tasks are prepared',\n",
       " 'The recorded metrics show that the results from section 4 are replicated']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f[\"requirements\"] for f in rubric_2[\"sub_tasks\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391d09fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "1: The Simformer architecture and training pipeline have been implemented as described in Section 3 (tokenizer, condition masks, structured attention masks, score-based diffusion training, sampling, and guided diffusion; Fig. 2).\n",
    "2: Benchmark posterior performance (C2ST) has been replicated on Linear Gaussian, Mixture Gaussian, Two Moons, and SLCP, showing Simformer matches or outperforms NPE and benefits from structured attention (Fig. 4a).\n",
    "3: Arbitrary-conditional inference accuracy (C2ST) has been replicated on Two Moons, SLCP, Tree, and HMM, demonstrating accurate sampling of diverse conditionals from a single model (Fig. 4b).\n",
    "4: Lotka–Volterra unstructured-observation inference has been replicated, including posterior and posterior predictive matching MCMC ground truth and uncertainty reduction with added predator observations ("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f319b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "1: The Simformer tokenizer, transformer with structured attention masks, and score-based diffusion training and sampling (Section 3) have been implemented.\n",
    "2: Figure 4a posterior C2ST benchmark results have been replicated, showing Simformer matches or outperforms NPE across tasks.\n",
    "3: Figure 4b arbitrary-conditional C2ST results have been replicated.\n",
    "4: Figure 5 Lotka–Volterra unstructured-observation posterior and posterior predictive results have been replicated.\n",
    "5: Figure 6 SIRD with time-varying contact rate (∞-dimensional parameter) posterior and parameter-conditioned inference have been replicated.\n",
    "6: Figure 7 Hodgkin–Huxley posterior and energy-interval constrained posterior via guided diffusion, including posterior predictives, have been replicated.\n",
    "7: Figure 3 demonstrations of sampling arbitrary conditionals of p(θ, x) have been replicated.\n",
    "8: The accuracy gains from exploiting simulator dependency structure via attention masks over dense masks (Fig. 4 and Appendix A5–A6) have been verified.\n",
    "9: The claim that Simformer achieves approximately 10× fewer simulations than NPE on average across benchmarks (Fig. 4a) has been verified.\n",
    "10: Appendix A5–A6 extended benchmarks (VESDE vs VPSDE; comparisons to NLE/NRE/NSPE) have been replicated.\n",
    "11: Appendix A7 evaluation of reverse SDE step budget effects on C2ST has been replicated.\n",
    "12: Appendix A8–A13 negative log-likelihood and calibration (expected coverage) analyses for posterior and likelihood have been replicated.\n",
    "13: Appendix A15–A16 diffusion guidance for interval and general constraints, including self-recurrence and RePaint baselines, has been replicated.\n",
    "14: Appendix A14 targeted inference with embedding networks on gravitational waves has been replicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d25234",
   "metadata": {},
   "outputs": [],
   "source": [
    "1: The Simformer joint diffusion–transformer model has been implemented to sample arbitrary conditionals (posterior, likelihood, and mixed parameter/data conditionals) from a single network.\n",
    "2: Benchmark superiority over NPE (and NLE/NRE) has been replicated on SBIBM tasks (Linear Gaussian, Gaussian Mixture, Two Moons, SLCP) in C2ST across simulation budgets, including gains from structured attention masks.\n",
    "3: Accurate sampling of arbitrary conditionals across tasks (Tree, HMM, Two Moons, SLCP) has been replicated, matching ground-truth conditionals in C2ST.\n",
    "4: Inference with unstructured/irregular observations in the Lotka–Volterra model has been replicated, including posterior and posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf0043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'1.1: The drift term for Variance Exploding SDE is defined as $f(x, t)=0$',\n",
    " '1.2: The diffusion term for Variance Exploding SDE is defined as $g(t)=\\\\sigma_{min} \\\\cdot (\\\\frac{\\\\sigma_{max}}{\\\\sigma_{min}})^t \\\\cdot \\\\sqrt{2 \\\\log \\\\frac{\\\\sigma_{max}}{\\\\sigma_{min}}}$',\n",
    " '1.3: The perturbation kernel for the Variance Exploding SDE is defined as $p(x_t|x_0) = \\\\mathcal{N}(x_t, x_t, \\\\sigma(t)I)$ where $\\\\sigma(t) = \\\\sigma_{min}^2\\\\left( \\\\frac{\\\\sigma_{max}}{\\\\sigma_{min}} \\\\right)^{2t}$',\n",
    " '1.4: The constant $\\\\sigma_{max}$ for VESDE is set to 15',\n",
    " '1.5: The constant $\\\\sigma_{min}$ for VESDE is set to 0.0001',\n",
    " '1.6: In VESDE, samples are drawn from the time interval [1e-5, 1]',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfebbdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.1: A tokenizer creates one token per variable (parameter or data) containing an identifier embedding, a value representation, and a condition-state embedding.\n",
    "1.2: Condition masks MC are supported to mark conditioned tokens and exclude them from noising during training and sampling.\n",
    "1.3: A transformer backbone is implemented with runtime-configurable attention masks ME.\n",
    "1.4: The attention mask system supports both symmetric (undirected) and asymmetric (directed) masks and allows dynamic updates when conditioning changes.\n",
    "1.5: A per-token score head with time embedding is implemented for a score-based diffusion model (VESDE or VPSDE).\n",
    "1.6: The denoising score-matching loss with partial corruption x̂_MC_t is implemented as defined in Section 3.3.\n",
    "1.7: The training loop samples MC across joint, posterior, likelihood, and random masks per batch and optimizes the model.\n",
    "1.8: A reverse SDE sampler generates unobserved tokens while holding conditioned tokens fixed to sample arbitrary conditionals.\n",
    "1.9: The trained network generates posterior samples p(θ|x) without retraining.\n",
    "1.10: The trained network generates likelihood samples p(x|θ) without retraining.\n",
    "1.11: The trained network generates mixed conditionals (e.g., subsets of parameters and data conditioned on others) without retraining."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecourtsvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
