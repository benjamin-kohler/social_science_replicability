"""Agent 4: Explainer - Analyzes and explains discrepancies in replications."""

import json
from pathlib import Path
from typing import Optional

from ..models.schemas import (
    PaperSummary,
    ReplicationResults,
    VerificationReport,
    ExplanationReport,
    DiscrepancyAnalysis,
    ReplicationGrade,
)
from ..models.config import Config
from ..utils.pdf_parser import extract_text_from_pdf
from ..utils.logging_utils import get_logger
from .base import BaseAgent

logger = get_logger(__name__)


EXPLAINER_SYSTEM_PROMPT = """You are an expert at analyzing research replication discrepancies.

Your task is to explain why replicated results might differ from original paper results.
You have access to:
1. The original paper
2. The methodological summary used for replication
3. The code generated by the replicator
4. The replicated results
5. The verification grades
6. The original replication package (if available)

Common causes of replication discrepancies:
- Different software implementations (Stata vs Python vs R)
- Ambiguous methodology descriptions
- Missing data processing steps
- Different random seeds
- Rounding differences
- Sample selection differences
- Version differences in statistical packages
- Typos or errors in original paper
- Undocumented data transformations

Your analysis should:
1. Identify the specific discrepancy
2. Propose likely causes
3. Determine if the cause is identifiable
4. Attribute fault (replicator error, paper ambiguity, or data issue)"""


DISCREPANCY_ANALYSIS_PROMPT = """Analyze the discrepancy for the following item.

## Item Information:
- Item ID: {item_id}
- Grade Received: {grade}
- Verification Notes: {verification_notes}

## Original Paper Section:
{original_section}

## Methodological Summary Used:
{method_summary}

## Replication Code:
{replication_code}

## Replicated Output:
{replicated_output}

## Original Replication Package (if available):
{replication_package}

Provide your analysis in this JSON format:
{{
    "item_id": "{item_id}",
    "grade": "{grade}",
    "description_of_discrepancy": "Detailed description of what differs",
    "likely_causes": ["List of possible causes, ordered by likelihood"],
    "is_identifiable": true/false,
    "fault_attribution": "replicator/original_paper/unclear/data_limitation",
    "confidence": "high/medium/low",
    "supporting_evidence": "Evidence supporting your analysis"
}}"""


class ExplainerAgent(BaseAgent):
    """Agent 4: Explains discrepancies in replication results.

    This agent analyzes non-A grades and provides explanations for
    why the replicated results differ from the original paper.
    """

    def __init__(self, config: Config):
        """Initialize the explainer agent.

        Args:
            config: Configuration object.
        """
        super().__init__(
            config=config,
            name="Explainer",
            role="replication discrepancy analyst",
            goal="Explain why replication results differ from originals",
        )

    def run(
        self,
        paper_path: str,
        paper_summary: PaperSummary,
        replication_results: ReplicationResults,
        verification_report: VerificationReport,
        replication_package_path: Optional[str] = None,
    ) -> ExplanationReport:
        """Generate explanations for replication discrepancies.

        Args:
            paper_path: Path to the original paper PDF.
            paper_summary: Methodological summary from Agent 1.
            replication_results: Results from Agent 2.
            verification_report: Grades from Agent 3.
            replication_package_path: Optional path to original replication package.

        Returns:
            ExplanationReport with analysis of each discrepancy.
        """
        logger.info(f"Analyzing discrepancies for: {verification_report.paper_id}")

        # Extract paper text
        paper_text = extract_text_from_pdf(paper_path)

        # Load replication package if available
        replication_package = self._load_replication_package(replication_package_path)

        # Analyze each non-A grade item
        analyses = []
        for item in verification_report.item_verifications:
            if item.grade != ReplicationGrade.A:
                analysis = self._analyze_discrepancy(
                    item=item,
                    paper_text=paper_text,
                    paper_summary=paper_summary,
                    replication_results=replication_results,
                    replication_package=replication_package,
                )
                analyses.append(analysis)

        # Generate overall assessment
        overall_assessment = self._generate_overall_assessment(
            analyses, verification_report
        )

        # Generate recommendations
        recommendations = self._generate_recommendations(analyses)

        # Compare with replication package if available
        package_comparison = None
        if replication_package:
            package_comparison = self._compare_with_package(
                replication_results, replication_package
            )

        report = ExplanationReport(
            paper_id=verification_report.paper_id,
            analyses=analyses,
            overall_assessment=overall_assessment,
            recommendations=recommendations,
            replication_package_comparison=package_comparison,
        )

        logger.info(f"Generated explanations for {len(analyses)} discrepancies")
        return report

    def _analyze_discrepancy(
        self,
        item,
        paper_text: str,
        paper_summary: PaperSummary,
        replication_results: ReplicationResults,
        replication_package: Optional[dict],
    ) -> DiscrepancyAnalysis:
        """Analyze a single discrepancy."""
        logger.info(f"Analyzing discrepancy for {item.item_id}")

        # Get relevant sections
        original_section = self._extract_relevant_section(paper_text, item.item_id)

        # Get method summary for this item
        method_summary = self._get_method_summary(paper_summary, item.item_id)

        # Get replication code for this item
        replication_code = self._get_replication_code(replication_results, item.item_id)

        # Get replicated output
        replicated_output = self._get_replicated_output(replication_results, item.item_id)

        # Get package code if available
        package_code = self._get_package_code(replication_package, item.item_id)

        # Generate analysis using LLM
        prompt = DISCREPANCY_ANALYSIS_PROMPT.format(
            item_id=item.item_id,
            grade=item.grade.value,
            verification_notes=item.comparison_notes,
            original_section=original_section[:3000],
            method_summary=method_summary[:2000],
            replication_code=replication_code[:3000],
            replicated_output=str(replicated_output)[:2000],
            replication_package=package_code[:2000] if package_code else "Not available",
        )

        try:
            response = self.generate_json(
                prompt=prompt,
                system_prompt=EXPLAINER_SYSTEM_PROMPT,
            )

            return DiscrepancyAnalysis(
                item_id=item.item_id,
                grade=item.grade,
                description_of_discrepancy=response.get("description_of_discrepancy", ""),
                likely_causes=response.get("likely_causes", []),
                is_identifiable=response.get("is_identifiable", False),
                fault_attribution=response.get("fault_attribution", "unclear"),
                confidence=response.get("confidence", "low"),
                supporting_evidence=response.get("supporting_evidence"),
            )

        except Exception as e:
            logger.error(f"Analysis failed for {item.item_id}: {e}")
            return DiscrepancyAnalysis(
                item_id=item.item_id,
                grade=item.grade,
                description_of_discrepancy=f"Analysis error: {e}",
                likely_causes=["Unable to determine"],
                is_identifiable=False,
                fault_attribution="unclear",
                confidence="low",
            )

    def _load_replication_package(
        self, package_path: Optional[str]
    ) -> Optional[dict]:
        """Load and index the original replication package."""
        if not package_path:
            return None

        package_dir = Path(package_path)
        if not package_dir.exists():
            logger.warning(f"Replication package not found: {package_path}")
            return None

        package = {"path": package_path, "files": {}}

        # Index common code files
        code_extensions = [".py", ".r", ".R", ".do", ".sas", ".m"]
        for ext in code_extensions:
            for file in package_dir.rglob(f"*{ext}"):
                try:
                    package["files"][str(file.relative_to(package_dir))] = file.read_text()
                except Exception as e:
                    logger.warning(f"Could not read {file}: {e}")

        logger.info(f"Loaded {len(package['files'])} files from replication package")
        return package

    def _extract_relevant_section(self, paper_text: str, item_id: str) -> str:
        """Extract relevant section of paper for an item."""
        import re

        # Find mentions of the item
        pattern = rf"({re.escape(item_id)}[^\n]*(?:\n(?![A-Z])[^\n]*)*)"
        matches = re.findall(pattern, paper_text, re.IGNORECASE)

        if matches:
            return "\n\n".join(matches[:5])

        # Fallback: return general context
        return paper_text[:3000]

    def _get_method_summary(self, summary: PaperSummary, item_id: str) -> str:
        """Get methodological summary for a specific item."""
        item_id_lower = item_id.lower()

        # Check tables
        for table in summary.tables:
            if table.table_number.lower() in item_id_lower:
                return json.dumps(table.model_dump(), indent=2)

        # Check figures
        for figure in summary.figures:
            if figure.figure_number.lower() in item_id_lower:
                return json.dumps(figure.model_dump(), indent=2)

        return "Item not found in methodological summary"

    def _get_replication_code(
        self, results: ReplicationResults, item_id: str
    ) -> str:
        """Get replication code for a specific item."""
        item_id_lower = item_id.lower()

        for code in results.code_files:
            if code.description and item_id_lower in code.description.lower():
                return code.code

        return "Code not found"

    def _get_replicated_output(
        self, results: ReplicationResults, item_id: str
    ) -> dict:
        """Get replicated output for a specific item."""
        item_id_lower = item_id.lower()

        # Check tables
        for table in results.tables:
            if table.table_number.lower() in item_id_lower:
                return table.data

        # Check figures
        for figure in results.figures:
            if figure.figure_number.lower() in item_id_lower:
                return {"file_path": figure.file_path}

        return {"error": "Output not found"}

    def _get_package_code(
        self, package: Optional[dict], item_id: str
    ) -> Optional[str]:
        """Get code from replication package for a specific item."""
        if not package:
            return None

        item_id_lower = item_id.lower()

        # Search package files for mentions of this item
        relevant_code = []
        for filename, content in package.get("files", {}).items():
            if item_id_lower in content.lower():
                relevant_code.append(f"--- {filename} ---\n{content}")

        return "\n\n".join(relevant_code) if relevant_code else None

    def _generate_overall_assessment(
        self,
        analyses: list[DiscrepancyAnalysis],
        verification: VerificationReport,
    ) -> str:
        """Generate overall assessment of the replication effort."""
        if not analyses:
            return "All items received grade A. Replication was fully successful."

        # Count attributions
        attributions = {}
        for a in analyses:
            attr = a.fault_attribution
            attributions[attr] = attributions.get(attr, 0) + 1

        # Summarize
        parts = [
            f"Of {len(verification.item_verifications)} items verified:",
            f"- {len(analyses)} had discrepancies requiring explanation",
        ]

        for attr, count in sorted(attributions.items(), key=lambda x: -x[1]):
            parts.append(f"- {count} attributed to: {attr}")

        # Assess overall success
        identifiable = sum(1 for a in analyses if a.is_identifiable)
        parts.append(f"\n{identifiable}/{len(analyses)} discrepancies have identifiable causes.")

        return "\n".join(parts)

    def _generate_recommendations(
        self, analyses: list[DiscrepancyAnalysis]
    ) -> list[str]:
        """Generate recommendations based on the analyses."""
        recommendations = []

        # Collect common causes
        all_causes = []
        for a in analyses:
            all_causes.extend(a.likely_causes)

        # Generate recommendations based on common patterns
        if any("software" in c.lower() for c in all_causes):
            recommendations.append(
                "Consider using the same statistical software as the original paper"
            )

        if any("ambiguous" in c.lower() or "unclear" in c.lower() for c in all_causes):
            recommendations.append(
                "Request clarification from original authors on ambiguous methodology"
            )

        if any("data" in c.lower() for c in all_causes):
            recommendations.append(
                "Verify that the same version of the data is being used"
            )

        # Default recommendations
        if not recommendations:
            recommendations = [
                "Review methodology descriptions for potential ambiguities",
                "Compare data processing steps in detail",
                "Check for version differences in statistical packages",
            ]

        return recommendations

    def _compare_with_package(
        self,
        results: ReplicationResults,
        package: dict,
    ) -> str:
        """Compare replication code with original package code."""
        comparison_parts = []

        for code in results.code_files:
            if not code.description:
                continue

            # Find matching package files
            for filename, pkg_code in package.get("files", {}).items():
                # Simple similarity check
                if any(var in pkg_code.lower() for var in ["table", "figure", "regression"]):
                    comparison_parts.append(
                        f"Potential match: {filename} may correspond to {code.description}"
                    )
                    break

        return "\n".join(comparison_parts) if comparison_parts else "No direct code matches found"
